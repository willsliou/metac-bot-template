1. Multiple Testing: The "Lindy Effect" for NewsThe Concept: If you search the internet for "Why will the S&P 500 go up?" you will find 100 different reasons. Even if the stock market is actually random, by sheer luck, 5 of those reasons will look like "genius" signals. This is Multiple Testing Bias.ELI5: If you ask 100 liars if it’s going to rain, at least 5 of them will probably guess right just by accident. You shouldn't trust them just because they were right once.The Bot Logic (Benjamini-Hochberg Filter):Instead of trusting the first "signal" the bot finds, we force it to find independent confirmation.Step A: The bot finds 10 "facts" (signals).Step B: It assigns a "p-value" (how likely is this just noise?) to each.Step C: It only keeps the signals that survive a threshold, ensuring we aren't just chasing "False Discoveries."Pythondef bh_filter(signals, alpha=0.05):
    """
    ELI5: Only keep signals that are 'strong enough' to 
    not be accidents.
    """
    # Sort signals by their 'noise' probability (p-value)
    sorted_signals = sorted(signals, key=lambda x: x['p_value'])
    m = len(sorted_signals)
    
    verified_signals = []
    for i, signal in enumerate(sorted_signals):
        # The Benjamini-Hochberg formula
        threshold = ((i + 1) / m) * alpha
        if signal['p_value'] <= threshold:
            verified_signals.append(signal)
            
    return verified_signals
2. Causal Inference: The "Engine" vs. The "Noise"The Concept: In Data 102, you use Directed Acyclic Graphs (DAGs). This helps you figure out if $X$ causes $Y$, or if $Z$ is just pulling the strings for both.ELI5: You notice that people who carry umbrellas are also more likely to get into car splashes. Does the umbrella cause the splash? No. The Rain (Confounder) causes both.The Bot Logic (The Causal Agent):Before your bot makes a prediction, force Gemini to "draw" the relationship.Question: "Will SPY go up because of the 2026 Tax Act?"Bot’s Causal Check: "Wait, is the Tax Act actually going to increase corporate profits (Causation), or is the market just going up because the Fed is printing money, and that same money printing is making politicians pass the Tax Act (Confounding)?"Action: If it’s just a confounder, your bot stays conservative. If it’s true causation, your bot goes aggressive.3. Concentration Inequalities: The "Seatbelt"The Concept: Algorithms like Hoeffding’s Inequality tell you how much your "sample average" might differ from the "true average."ELI5: If you flip a coin 3 times and get 3 heads, you might think the coin is broken. But if you flip it 1,000 times and get 550 heads, you are much more certain it’s biased. The "Inequality" is the math that tells you how much to trust your sample.The Bot Logic (Calibration):On Metaculus, the worst thing you can be is "Wrong and Confident."$$P(|\bar{X} - E[X]| \geq \epsilon) \leq 2e^{-2n\epsilon^2}$$How to use this: If your bot only finds 3 news articles about a topic, $n$ is small. The "Concentration" is weak.The Adjustment: The bot should "shrink" its prediction toward 50%. It says: "I think it's 80%, but my sample size is tiny, so I’ll output 60% to be safe."4. Loss Functions: Playing to WinThe Concept: Data 102 teaches you that you don't always want to be "accurate"; you want to minimize Expected Loss.ELI5: If I bet you $100 that it won't rain, and the "cost" of being wrong is that your house floods, you shouldn't take the bet even if there's only a 1% chance of rain. The "Loss" is too high.The Bot Logic:Metaculus uses a Logarithmic Score.If you predict 99% and it's No, you lose ~6.6 points.If you predict 55% and it's No, you only lose ~0.8 points.The Strategy: Your bot should be "Risk Averse." It should only move to extreme percentages (like 90%+) if the Causal Engine (Point #2) and the Verified Signals (Point #1) are both 100% aligned.
The Concept: In Data 102, you use Directed Acyclic Graphs (DAGs). This helps you figure out if $X$ causes $Y$, or if $Z$ is just pulling the strings for both.ELI5: You notice that people who carry umbrellas are also more likely to get into car splashes. Does the umbrella cause the splash? No. The Rain (Confounder) causes both.The Bot Logic (The Causal Agent):Before your bot makes a prediction, force Gemini to "draw" the relationship.Question: "Will SPY go up because of the 2026 Tax Act?"Bot’s Causal Check: "Wait, is the Tax Act actually going to increase corporate profits (Causation), or is the market just going up because the Fed is printing money, and that same money printing is making politicians pass the Tax Act (Confounding)?"Action: If it’s just a confounder, your bot stays conservative. If it’s true causation, your bot goes aggressive.3. Concentration Inequalities: The "Seatbelt"The Concept: Algorithms like Hoeffding’s Inequality tell you how much your "sample average" might differ from the "true average."ELI5: If you flip a coin 3 times and get 3 heads, you might think the coin is broken. But if you flip it 1,000 times and get 550 heads, you are much more certain it’s biased. The "Inequality" is the math that tells you how much to trust your sample.The Bot Logic (Calibration):On Metaculus, the worst thing you can be is "Wrong and Confident."$$P(|\bar{X} - E[X]| \geq \epsilon) \leq 2e^{-2n\epsilon^2}$$How to use this: If your bot only finds 3 news articles about a topic, $n$ is small. The "Concentration" is weak.The Adjustment: The bot should "shrink" its prediction toward 50%. It says: "I think it's 80%, but my sample size is tiny, so I’ll output 60% to be safe."4. Loss Functions: Playing to WinThe Concept: Data 102 teaches you that you don't always want to be "accurate"; you want to minimize Expected Loss.ELI5: If I bet you $100 that it won't rain, and the "cost" of being wrong is that your house floods, you shouldn't take the bet even if there's only a 1% chance of rain. The "Loss" is too high.The Bot Logic:Metaculus uses a Logarithmic Score.If you predict 99% and it's No, you lose ~6.6 points.If you predict 55% and it's No, you only lose ~0.8 points.The Strategy: Your bot should be "Risk Averse." It should only move to extreme percentages (like 90%+) if the Causal Engine (Point #2) and the Verified Signals (Point #1) are both 100% aligned.










A. The "False Discovery Rate" (FDR) & Multiple Testing
In Data 102, you learn about Benjamini-Hochberg and controlling for false positives.

The Bot Problem: If your bot searches the web for "reasons SPY will go up," it will find 1,000 correlations. If it tests enough of them, it will find one that looks "significant" (e.g., "The S&P 500 always rises when a specific movie franchise releases a sequel").

The Application: Use Shrinkage. When your bot finds a "signal," don't let it move the probability from 50% to 90% immediately. Apply a "penalty" for multiple comparisons to keep your forecasts closer to the base rate unless the evidence is overwhelming.

B. Causal Inference (Potential Outcomes & Confounders)
This is exactly what we discussed regarding Bridgewater. Data 102 teaches the Rubin Causal Model and Directed Acyclic Graphs (DAGs).

The Bot Problem: Distinguishing between a "neighboring" event and a "causing" event.

The Application: Force Gemini to draw a "Mental DAG."

Prompt logic: "Identify the Confounder. Is the 2026 Tax Act causing the SPY rise, or is 'General Economic Optimism' causing both the Tax Act to pass AND the SPY to rise?"

If the Tax Act is a mediator, the bot should focus its research on the bill's progress, not the stock price.

C. Bayesian Decision Theory & Loss Functions
Data 102 focuses on minimizing "Risk" (Expected Loss).

The Bot Problem: Metaculus uses a Logarithmic Scoring Rule. This is a specific "Loss Function" where being "Wrong and Confident" (e.g., predicting 99% on something that becomes a No) is catastrophic for your rank.

The Application: Tell the bot: "Your Loss Function is the Metaculus Peer Score. Minimize the risk of being in the 'Long Tail' of the distribution." This forces the bot to avoid 0% and 100% predictions unless the event is mathematically certain.





Since you’ve taken Data 102 (Data, Inference, and Decisions) at Berkeley, you’ve actually already been trained in the exact "Decision Engine" required for high-level forecasting. That course moves beyond just "finding patterns" (Data 100) to making choices under uncertainty.Here are the four key "Mental Models" from Data 102 that we can drop directly into your bot’s Python logic:1. The Multi-Armed Bandit (Exploration vs. Exploitation)In Data 102, you learned about Thompson Sampling and Upper Confidence Bounds (UCB).The Concept: When you have multiple ways to get information (e.g., searching news, looking at historical price data, or asking an LLM to simulate a scenario), how do you decide which one to trust?Application to your Bot: Instead of running the same "Research" code for every question, your bot can act as a "Bandit." It can try different research strategies and, over time, "Exploit" the ones that have led to the most accurate predictions in your backtesting.2. False Discovery Rate (FDR) & Multiple TestingYou likely remember the Benjamini-Hochberg procedure.The Concept: If you test 100 different "correlations" (like our S&P 500 example), by pure chance, 5 of them will look statistically significant even if they are garbage.Application to your Bot: Your bot should not bet the house on a single "signal" it finds in the news. You can implement an FDR-style filter: before the bot updates its forecast based on a "new fact," it must verify that fact across multiple independent sources. If only one source mentions the "2026 Tax Act," the bot should treat it as a "False Discovery" and ignore it.3. Causal Inference (SCMs and Potential Outcomes)This is the heart of what we discussed regarding "Bridgewater." Data 102 teaches you to draw Directed Acyclic Graphs (DAGs) to identify Confounders and Colliders.The Concept: You can’t just regress $Y$ on $X$ if a third variable $Z$ causes both.Application to your Bot: In your "Inside View" agent, force the LLM to identify the DAG.Question: "Does the Maduro capture cause the oil price drop, or are they both caused by a third factor (US Foreign Policy shift)?"By identifying the Structural Causal Model, the bot can predict what happens if the "cause" changes (e.g., what if the US policy shifts again?).4. Concentration Inequalities (Tail Risk)Remember Hoeffding’s Inequality or Chernoff Bounds?The Concept: These provide a mathematical "guarantee" of how far a random variable can deviate from its mean.Application to your Bot: Metaculus rewards "Calibration." If your bot is 99% sure but is wrong, you lose massive points. You can use these inequalities to "bound" your bot's confidence. If the news is thin, the bot should use a Chernoff Bound to force its probability closer to 50% (the "uninformed" state) to protect your leaderboard rank from "Black Swan" events.How this looks in your Python TemplateTo apply these, you would update the reasoning loop in your main.py to include a "Causal Check" step:Python# A Data 102 inspired 'Causal Agent'
def causal_inference_check(news_items, question):
    prompt = f"""
    Identify the Causal DAG for these news items: {news_items}
    1. What is the Treatment (X)?
    2. What is the Outcome (Y)?
    3. Are there Confounders (Z)?
    4. Is the correlation between X and Y spurious?
    """
    return gemini_model.ask(prompt)
By adding this, your bot isn't just "predicting"—it's inferring.Predicting with Historical DataThis video discusses how to move beyond simple data mining to find true predictive capacity in historical data, which is a core theme of the inference and decision-making principles taught in Data 102.Would you like me to write the code for a 'Benjamini-Hochberg' filter that your bot can use to verify its search results?



